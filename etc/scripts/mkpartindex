#!/usr/bin/env python
# SPDX-License-Identifier: BSD-3-Clause
# This script generates the Xylene part index
import sys
import multiprocessing
import re
import logging     as log
import json
from datetime      import datetime

from xylene.common import (
	general_init, init_cli,

	XYLENE_SCRIPTS,
	XYLENE_WORKING_DIR,
	XYLENE_DB_DIR
)

from xylene.vivado import Vivado

from rich.progress import (
	Progress, SpinnerColumn, BarColumn,
	TextColumn
)

RAW_PART_DB       = (XYLENE_WORKING_DIR / 'raw-parts.json')
NORMALIZED_INDEX  = (XYLENE_DB_DIR / 'part_index.json')
TILE_WORKING_DIR  = (XYLENE_WORKING_DIR / 'tile_extraction')
TILE_SCRIPT_DIR   = (TILE_WORKING_DIR / 'tcl')
TILE_OUTPUT_DIR   = (TILE_WORKING_DIR / 'json')

def dump_parts(args):
	vivado = Vivado()

	vivado.run_tcl(
		tcl_file = f'{XYLENE_SCRIPTS}/dump-parts.tcl',
		tcl_args = [str(RAW_PART_DB)]
	)

def sanitize(args):
	log.info('Stripping trailing commas')

	with open(RAW_PART_DB, 'r') as raw_parts:
		raw = re.sub(
			r'\"\,\n\t\t\}',
			'\"\n\t\t}',
			raw_parts.read()
		)
	with open(RAW_PART_DB, 'w') as raw_parts:
		raw_parts.write(
			re.sub(
				r'\}\,\n\t\]\n\}',
				'}\n\t]\n}',
				raw
			)
		)

def normalize(args):
	log.info('Reading raw index')
	with open(RAW_PART_DB, 'r') as raw:
		data = json.load(raw)

	log.info(f'Raw index generated with Vivado {data["vivado_version"]}')

	index = {
		'version'  : data['vivado_version'],
		'generated': datetime.now().isoformat(),
		'families' : {},
		'devices'  : {},
		'packages' : {},
	}

	part_cnt = 0
	log.info('Normalizing')
	for part in data['parts']:
		family      = part['FAMILY']
		package     = part['PACKAGE']
		device_name = part['DEVICE']

		if not family in index['families']:
			log.info(f'New family found: \'{family}\', normalizing name')
			arch_name = part['ARCHITECTURE_FULL_NAME']
			if family[0] == 'q':
				arch_name = f'Defense-Grade {arch_name}'

			if family[0] == 'a' and family[1] != 'r':
				arch_name = f'Automotive {arch_name}'

			if family[-1:] == 'l':
				arch_name = f'{arch_name} Low Voltage'

			if family[-3:] == 'HBM':
				arch_name = f'{arch_name} HBM'

			if family[-3:] == 'es1':
				arch_name = f'{arch_name} ES1'

			log.info(f'Name was: \'{part["ARCHITECTURE_FULL_NAME"]}\', is now \'{arch_name}\'')
			index['families'][family] = {
				'full_name': arch_name
			}

		if not package in index['packages']:
			log.info(f'New package found: \'{package}\'')
			index['packages'][package] = []

		if not device_name in index['packages'][package]:
			index['packages'][package].append(device_name)

		if not 'devices' in index['families'][family]:
			index['families'][family]['devices'] = []

		if not device_name in index['families'][family]['devices']:
			index['families'][family]['devices'].append(device_name)

		dev_data = {
			'family': family,
			'speed_variants': [
				{
					'grade': part['SPEED'],
					'label': part['SPEED_LABEL'],
					'level_id': {
						'id': part['SPEED_LEVEL_ID'],
						'date': part['SPEED_LEVEL_ID_DATE']
					}
				}
			],
			'iob' : int(part['AVAILABLE_IOBS']),
			'bram': int(part['BLOCK_RAMS']),
			'dsp' : int(part['DSP']),
			'ff'  : int(part['FLIPFLOPS']),
			'lut' : int(part['LUT_ELEMENTS']),
			'transceivers': {
				'GB'   : 0 if 'GB_TRANSCEIVERS'    not in part else int(part['GB_TRANSCEIVERS']   ),
				'GTHE2': 0 if 'GTHE2_TRANSCEIVERS' not in part else int(part['GTHE2_TRANSCEIVERS']),
				'GTHE3': 0 if 'GTHE3_TRANSCEIVERS' not in part else int(part['GTHE3_TRANSCEIVERS']),
				'GTHE4': 0 if 'GTHE4_TRANSCEIVERS' not in part else int(part['GTHE4_TRANSCEIVERS']),
				'GTM'  : 0 if 'GTM_TRANSCEIVERS'   not in part else int(part['GTM_TRANSCEIVERS']  ),
				'GTME5': 0 if 'GTME5_TRANSCEIVERS' not in part else int(part['GTME5_TRANSCEIVERS']),
				'GTPE2': 0 if 'GTPE2_TRANSCEIVERS' not in part else int(part['GTPE2_TRANSCEIVERS']),
				'GTXE2': 0 if 'GTXE2_TRANSCEIVERS' not in part else int(part['GTXE2_TRANSCEIVERS']),
				'GTYE3': 0 if 'GTYE3_TRANSCEIVERS' not in part else int(part['GTYE3_TRANSCEIVERS']),
				'GTYE4': 0 if 'GTYE4_TRANSCEIVERS' not in part else int(part['GTYE4_TRANSCEIVERS']),
				'GTYE5': 0 if 'GTYE5_TRANSCEIVERS' not in part else int(part['GTYE5_TRANSCEIVERS']),
				'GTYP' : 0 if 'GTYP_TRANSCEIVERS'  not in part else int(part['GTYP_TRANSCEIVERS'] ),
				'GTZE2': 0 if 'GTZE2_TRANSCEIVERS' not in part else int(part['GTZE2_TRANSCEIVERS']),
			},

			'slices': int(part['SLICES']),

			'io': {
				'pins'     : int(part['IO_PIN_COUNT']),
				'standards': part['IO_STANDARDS'].split(' ')
			},

			'cols'      : int(part['COLS']),
			'rows'      : int(part['ROWS']),
			'compatible': part['COMPATIBLE_PARTS'].split(' '),
			'temp_variants': [
				{
					'grade'   : part['TEMPERATURE_GRADE_LETTER'],
					'ref_temp': int(part['REF_OPERATING_TEMPERATURE']),
					'max_temp': int(part['MAX_OPERATING_TEMPERATURE']),
				}
			],
		}

		if not device_name in index['devices']:
			if dev_data['temp_variants'][0]['grade'] == '':
				dev_data['temp_variants'].remove(dev_data['temp_variants'][0])

			if dev_data['speed_variants'][0]['grade'] == '':
				dev_data['speed_variants'].remove(dev_data['speed_variants'][0])

			index['devices'][device_name] = dev_data
		else:

			temp_variant = {
				'grade'   : part['TEMPERATURE_GRADE_LETTER'],
				'ref_temp': int(part['REF_OPERATING_TEMPERATURE']),
				'max_temp': int(part['MAX_OPERATING_TEMPERATURE']),
			}

			speed_variant = {
				'grade': part['SPEED'],
				'label': part['SPEED_LABEL'],
				'level_id': {
					'id': part['SPEED_LEVEL_ID'],
					'date': part['SPEED_LEVEL_ID_DATE']
				}
			}

			if not temp_variant['grade'] == '':
				if not temp_variant in index['devices'][device_name]['temp_variants']:
					index['devices'][device_name]['temp_variants'].append(temp_variant)

			if not speed_variant['grade'] == '':
				if not speed_variant in index['devices'][device_name]['speed_variants']:
					index['devices'][device_name]['speed_variants'].append(speed_variant)

		part_cnt += 1


	log.info(f'Normalized {len(index["devices"])} devices in {len(index["families"])} families and {len(index["packages"])} packages from {part_cnt} parts')

	return index


def write_family_json(args, norm):
	families = ('spartan', 'artix', 'virtex', 'kintex', 'zynq', 'versal')

	for family in families:

		f_names = filter(lambda f: family in f, norm['families'])
		family_devices = []

		f_index = {
			'version'  : norm['version'],
			'generated': datetime.now().isoformat(),
			'families' : {},
			'devices'  : {},
			'packages' : {},
		}

		for f in f_names:
			family_devices += norm['families'][f]['devices']

			f_index['families'][f] = norm['families'][f]
			for dev in family_devices:
				f_index['devices'][dev] = norm['devices'][dev]

		for dev in f_index['devices'].keys():
			for (pkg_name, devs) in norm['packages'].items():
				if dev in devs:
					if not pkg_name in f_index['packages']:
						f_index['packages'][pkg_name] = []
					f_index['packages'][pkg_name].append(dev)


		log.info(f'Writing index for \'{family}\' ({len(f_index["devices"])} devices)')
		with open(((XYLENE_DB_DIR / family) / 'part_index.json'), 'w') as index:
			json.dump(f_index, index)


def _run_vivado(tcl_file):
	log.info(f'Running {tcl_file}')
	with Vivado(log = False, journal = False) as v:
		v.run_tcl(
			tcl_file = tcl_file,
			tcl_args = [str((TILE_OUTPUT_DIR / tcl_file.replace('tcl', 'json')).resolve())]
		)

def extract_tiles(args, norm):
	def batch(tcl_files, jobs = multiprocessing.cpu_count() // 4):

		with multiprocessing.Pool(jobs) as p:
			p.map(_run_vivado, tcl_files)

	families = ('spartan', 'artix', 'virtex', 'kintex', 'zynq', 'versal')

	f_devs = {}
	f_map  = {}

	if not TILE_WORKING_DIR.exists():
		TILE_WORKING_DIR.mkdir()

	if not TILE_SCRIPT_DIR.exists():
		TILE_SCRIPT_DIR.mkdir()

	if not TILE_OUTPUT_DIR.exists():
		TILE_OUTPUT_DIR.mkdir()

	for family in families:
		if not family in f_devs:
			f_devs[family] = []
			f_map[family] = {}


		f_names = filter(lambda f: family in f, norm['families'])
		for f in f_names:
			f_devs[family] += norm['families'][f]['devices']

	for (pkg, devs) in norm['packages'].items():
		log.info(f'{len(devs)} devices have a {pkg} package')
		for family in families:
			pkg_devs = list(filter(lambda d: d in f_devs[family], devs))
			if len(pkg_devs) > 0:
				log.info(f'Found {len(pkg_devs)} devices in {family} that have a {pkg} package')
				if not pkg in f_map[family]:
					f_map[family][pkg] = []
				f_map[family][pkg] += pkg_devs

	scripts = []

	for (fam, pkgs) in f_map.items():
		log.info(f'Family {fam} has {len(pkgs)} known packages')
		for (pkg, devs) in pkgs.items():
			script = (TILE_SCRIPT_DIR / f'{fam}-{pkg}.tcl')
			with open(script, 'w') as s:
				s.write(f'''\
#########################################
# THIS FILE WAS AUTOGENERATED BY XYLENE #
# { datetime.now().isoformat().center(37) } #
#          !!! DO NOT EDIT !!!          #
#########################################

proc extract_tiles {{ fname }} {{
	set file [open $fname "w+"]

	puts $file "{{"
				''')
				for d_name in devs:
					d = norm['devices'][d_name]
					s.write(f'''
	create_project -force -in_memory -name {fam}_{d_name}_{pkg} -part {d_name}{pkg}{d['speed_variants'][0]['grade']}

	set_property DESIGN_MODE PinPlanning [get_filesets sources_1]
	open_io_design

	if {{![catch {{set tiles [get_tiles -verbose]}} nyeh]}} {{
		puts $file "\t\\"{d_name}_{pkg}\\": \\["

		foreach tile $tiles {{
			puts $file "\t\t\\"$tile\\","
		}}

		puts $file "\t\\],"
	}}

					''')
				s.write('''
	puts $file "}"
}
if { $argc == 1 } {
	set fname [lindex $argv 0]
	extract_tiles $fname
} {
	error "Specify a file name"
	exit
}
exit
				''')
				s.flush()

			scripts.append(str(script.resolve()))
	batch(scripts)

def main():
	general_init()

	parser = init_cli(
		'mkpartindex',
		'Generate the Xylene device index'
	)

	args = parser.parse_args()

	with Progress(
		SpinnerColumn(),
		TextColumn('[progress.description]{task.description}'),
		BarColumn(bar_width = None),
		transient = True
	) as progress:
		task = progress.add_task('Extracting Parts', start=False)
		if not RAW_PART_DB.exists():
			log.info('Generating raw part index, this will take a [yellow][i]while[/][/]', extra = { 'markup': True })
			dump_parts(args)

		progress.update(task, description = 'Sanitizing raw index')
		sanitize(args)

		progress.update(task, description = 'Normalizing index')
		norm = normalize(args)

		progress.update(task, description = 'Writing monolithic JSON index')

		# with open(NORMALIZED_INDEX, 'w') as index:
		# 	json.dump(norm, index)



		progress.update(task, description = 'Extracting tile map')
		# extract_tiles(args, norm)

		progress.update(task, description = 'Writing finalized monolithic JSON index')

		with open(NORMALIZED_INDEX, 'w') as index:
			json.dump(norm, index)

		progress.update(task, description = 'Writing independent family JSON indices')
		write_family_json(args, norm)
	return 0



if __name__ == '__main__':
	sys.exit(main())
