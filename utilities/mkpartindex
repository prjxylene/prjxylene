#!/usr/bin/env python
# SPDX-License-Identifier: BSD-3-Clause
# This script generates the Xylene part index
import sys
import multiprocessing
import re
import logging     as log
import json
from datetime      import datetime
from pathlib       import Path

from prjxylene.common import (
	main,

	XYLENE_WORKING_DIR,
	XYLENE_DB_DIR
)

from prjxylene.vivado import Vivado

from prjxylene.templates import (
	render_template
)

from rich.progress import (
	Progress, SpinnerColumn, BarColumn,
	TextColumn
)

RAW_PART_INDEX    = (XYLENE_WORKING_DIR / 'raw-parts.json')
NORMALIZED_INDEX  = (XYLENE_DB_DIR      / 'part_index.json')
TILE_WORKING_DIR  = (XYLENE_WORKING_DIR / 'tile_extraction')
TILE_SCRIPT_DIR   = (TILE_WORKING_DIR   / 'tcl')
TILE_OUTPUT_DIR   = (TILE_WORKING_DIR   / 'json')
TILE_LOG_DIR      = (TILE_WORKING_DIR   / 'logs')



def normalize(args):
	log.info('Reading raw index')
	with open(RAW_PART_INDEX, 'r') as raw:
		data = json.load(raw)

	log.info(f'Raw index generated with Vivado {data["vivado_version"]}')

	index = {
		'version'  : data['vivado_version'],
		'generated': datetime.now().isoformat(),
		'families' : {},
		'devices'  : {},
		'packages' : {},
	}

	part_cnt = 0
	log.info('Normalizing')
	for part in data['parts']:
		if len(part) == 0:
			continue

		family        = part['FAMILY']
		package       = part['PACKAGE']
		device_name   = part['DEVICE']
		part_subclass = ''

		if family[0] == 'q':
			part_subclass = 'defense-grade'
		elif family[0] == 'a':
			part_subclass = 'automotive'
		elif family[-1:] == 'l':
			part_subclass = 'low-voltage'
		else:
			part_subclass = 'commercial'

		if family not in index['families']:
			log.info(f'New family found: \'{family}\', normalizing name')
			arch_name = part['ARCHITECTURE_FULL_NAME']
			if family[0] == 'q':
				arch_name = f'Defense-Grade {arch_name}'

			if family[0] == 'a' and family[1] != 'r':
				arch_name = f'Automotive {arch_name}'

			if family[-1:] == 'l':
				arch_name = f'{arch_name} Low Voltage'

			if family[-3:] == 'HBM':
				arch_name = f'{arch_name} HBM'

			if family[-3:] == 'es1':
				arch_name = f'{arch_name} ES1'

			log.info(f'Name was: \'{part["ARCHITECTURE_FULL_NAME"]}\', is now \'{arch_name}\'')
			index['families'][family] = {
				'full_name': arch_name
			}

		if package not in index['packages']:
			log.info(f'New package found: \'{package}\'')
			index['packages'][package] = []

		if device_name not in index['packages'][package]:
			index['packages'][package].append(device_name)

		if 'devices' not in index['families'][family]:
			index['families'][family]['devices'] = []

		if device_name not in index['families'][family]['devices']:
			index['families'][family]['devices'].append(device_name)

		dev_data = {
			'family': family,
			'subclass': part_subclass,
			'speed_variants': [
				{
					'grade': part['SPEED'],
					'label': part['SPEED_LABEL'],
					'level_id': {
						'id': part['SPEED_LEVEL_ID'],
						'date': part['SPEED_LEVEL_ID_DATE']
					}
				}
			],
			'iob' : int(part['AVAILABLE_IOBS']),
			'bram': int(part['BLOCK_RAMS']),
			'dsp' : int(part['DSP']),
			'ff'  : int(part['FLIPFLOPS']),
			'lut' : int(part['LUT_ELEMENTS']),
			'transceivers': {
				'GB'   : 0 if 'GB_TRANSCEIVERS'    not in part else int(part['GB_TRANSCEIVERS']   ),
				'GTHE2': 0 if 'GTHE2_TRANSCEIVERS' not in part else int(part['GTHE2_TRANSCEIVERS']),
				'GTHE3': 0 if 'GTHE3_TRANSCEIVERS' not in part else int(part['GTHE3_TRANSCEIVERS']),
				'GTHE4': 0 if 'GTHE4_TRANSCEIVERS' not in part else int(part['GTHE4_TRANSCEIVERS']),
				'GTM'  : 0 if 'GTM_TRANSCEIVERS'   not in part else int(part['GTM_TRANSCEIVERS']  ),
				'GTME5': 0 if 'GTME5_TRANSCEIVERS' not in part else int(part['GTME5_TRANSCEIVERS']),
				'GTPE2': 0 if 'GTPE2_TRANSCEIVERS' not in part else int(part['GTPE2_TRANSCEIVERS']),
				'GTXE2': 0 if 'GTXE2_TRANSCEIVERS' not in part else int(part['GTXE2_TRANSCEIVERS']),
				'GTYE3': 0 if 'GTYE3_TRANSCEIVERS' not in part else int(part['GTYE3_TRANSCEIVERS']),
				'GTYE4': 0 if 'GTYE4_TRANSCEIVERS' not in part else int(part['GTYE4_TRANSCEIVERS']),
				'GTYE5': 0 if 'GTYE5_TRANSCEIVERS' not in part else int(part['GTYE5_TRANSCEIVERS']),
				'GTYP' : 0 if 'GTYP_TRANSCEIVERS'  not in part else int(part['GTYP_TRANSCEIVERS'] ),
				'GTZE2': 0 if 'GTZE2_TRANSCEIVERS' not in part else int(part['GTZE2_TRANSCEIVERS']),
			},

			'slices': int(part['SLICES']),

			'io': {
				'pins'     : int(part['IO_PIN_COUNT']),
				'standards': part['IO_STANDARDS'].split(' ')
			},

			'cols'      : int(part['COLS']),
			'rows'      : int(part['ROWS']),
			'compatible': part['COMPATIBLE_PARTS'].split(' '),
			'temp_variants': [
				{
					'grade'   : part['TEMPERATURE_GRADE_LETTER'],
					'ref_temp': int(part['REF_OPERATING_TEMPERATURE']),
					'max_temp': int(part['MAX_OPERATING_TEMPERATURE']),
				}
			],
		}

		if device_name not in index['devices']:
			if dev_data['temp_variants'][0]['grade'] == '':
				dev_data['temp_variants'].remove(dev_data['temp_variants'][0])

			if dev_data['speed_variants'][0]['grade'] == '':
				dev_data['speed_variants'].remove(dev_data['speed_variants'][0])

			index['devices'][device_name] = dev_data
		else:

			temp_variant = {
				'grade'   : part['TEMPERATURE_GRADE_LETTER'],
				'ref_temp': int(part['REF_OPERATING_TEMPERATURE']),
				'max_temp': int(part['MAX_OPERATING_TEMPERATURE']),
			}

			speed_variant = {
				'grade': part['SPEED'],
				'label': part['SPEED_LABEL'],
				'level_id': {
					'id': part['SPEED_LEVEL_ID'],
					'date': part['SPEED_LEVEL_ID_DATE']
				}
			}

			if temp_variant['grade'] != '':
				if temp_variant not in index['devices'][device_name]['temp_variants']:
					index['devices'][device_name]['temp_variants'].append(temp_variant)

			if not speed_variant['grade'] != '':
				if speed_variant not in index['devices'][device_name]['speed_variants']:
					index['devices'][device_name]['speed_variants'].append(speed_variant)

		part_cnt += 1


	log.info(f'Normalized {len(index["devices"])} devices in {len(index["families"])} families and {len(index["packages"])} packages from {part_cnt} parts')

	return index


def write_family_json(args, norm):
	families = ('spartan', 'artix', 'virtex', 'kintex', 'zynq', 'versal')

	for family in families:

		f_names = filter(lambda f: family in f, norm['families'])
		family_devices = []

		f_index = {
			'version'  : norm['version'],
			'generated': datetime.now().isoformat(),
			'families' : {},
			'devices'  : {},
			'packages' : {},
		}

		for f in f_names:
			family_devices += norm['families'][f]['devices']

			f_index['families'][f] = norm['families'][f]
			for dev in family_devices:
				f_index['devices'][dev] = norm['devices'][dev]

		for dev in f_index['devices'].keys():
			for (pkg_name, devs) in norm['packages'].items():
				if dev in devs:
					if pkg_name not in f_index['packages']:
						f_index['packages'][pkg_name] = []
					f_index['packages'][pkg_name].append(dev)


		log.info(f'Writing index for \'{family}\' ({len(f_index["devices"])} devices)')
		with open(((XYLENE_DB_DIR / family) / 'part_index.json'), 'w') as index:
			json.dump(f_index, index)


def _run_vivado(tcl_file):
	out_file = (TILE_OUTPUT_DIR / tcl_file.replace('tcl', 'json')).resolve()
	tcl_file_path = Path(tcl_file)
	if out_file.exists():
		log.info(f'Skipping {tcl_file_path.name}, output file {out_file.name} exists')
		return

	log.info(f'Running {tcl_file}')
	with Vivado(log = False, journal = False) as v:
		ret = v.run_tcl(
			tcl_file = tcl_file,
			tcl_args = [str(out_file)]
		)
		if not ret:
			if out_file.exists():
				out_file.unlink()

			log.error(f'Running {tcl_file_path.name} failed!')
			log.info('Writing process output logs')

			with open((TILE_LOG_DIR / f'{tcl_file_path.name}.stdout'), 'w+') as stdout:
				stdout.write(v.stdout.decode('utf-8'))

			with open((TILE_LOG_DIR / f'{tcl_file_path.name}.stderr'), 'w+') as stderr:
				stderr.write(v.stderr.decode('utf-8'))
		else:
			log.info(f'Tile maps written to \'{out_file.name}\'')
			log.info(f'Checking for valid JSON data in \'{out_file.name}\'')
			try:
				with open(out_file, 'r') as of:
					json.load(of)
				log.info(f'\'{out_file.name}\' is a valid json file')
			except: # noqa: E722
				log.error(f'The file \'{out_file.name}\' does not contain valid json')

				with open((TILE_LOG_DIR / f'{tcl_file_path.name}.stdout'), 'w+') as stdout:
					stdout.write(v.stdout.decode('utf-8'))

				with open((TILE_LOG_DIR / f'{tcl_file_path.name}.stderr'), 'w+') as stderr:
					stderr.write(v.stderr.decode('utf-8'))

def extract_tiles(args, norm):
	families = ('spartan', 'artix', 'virtex', 'kintex', 'zynq', 'versal')

	f_devs = {}
	f_map  = {}

	if not TILE_WORKING_DIR.exists():
		TILE_WORKING_DIR.mkdir()

	if not TILE_SCRIPT_DIR.exists():
		TILE_SCRIPT_DIR.mkdir()

	if not TILE_LOG_DIR.exists():
		TILE_LOG_DIR.mkdir()

	if not TILE_OUTPUT_DIR.exists():
		TILE_OUTPUT_DIR.mkdir()

	for family in families:
		if family not in f_devs:
			f_devs[family] = []
			f_map[family] = {}


		f_names = filter(lambda f: family in f, norm['families'])
		for f in f_names:
			f_devs[family] += norm['families'][f]['devices']

	for (pkg, devs) in norm['packages'].items():
		log.info(f'{len(devs)} devices have a {pkg} package')
		for family in families:
			pkg_devs = list(filter(lambda d: d in f_devs[family], devs))
			if len(pkg_devs) > 0:
				log.info(f'Found {len(pkg_devs)} devices in {family} that have a {pkg} package')
				if pkg not in f_map[family]:
					f_map[family][pkg] = []
				f_map[family][pkg] += pkg_devs

	scripts = []

	def _gen_partnumber(part_tuple):
		name        = part_tuple[0]
		package     = part_tuple[1]
		dev_dict    = part_tuple[2]
		part_number = ''

		grade        = dev_dict['speed_variants'][0]['grade']

		# Construct valid part number
		if name[3] == 'u':
			# UltraScale or UltraScale+
			temp_variant = dev_dict['temp_variants'][0]['grade'].lower()
			part_number = f'{name}-{package}{grade}-{temp_variant}'
		elif name[:3] == 'xcu':
			# Alveo devices
			temp_variant = dev_dict['temp_variants'][0]['grade'].lower()
			part_number = f'{name}-{package}{grade}-{temp_variant}'
		elif name == 'xqrvc1902' and package == 'vsra2197':
			# The xqrvc1902-vsra2197 only has -1MM-b-S
			part_number = f'{name}-{package}-1MM-b-S'
		elif name[:3] == 'xqr':
			# TODO: Rad-hard Devices
			temp_variant = dev_dict['temp_variants'][0]['grade'].lower()
			part_number = f'{name}-{package}{grade}-{temp_variant}'
		elif name[:4] in ('xcvm', 'xcvc', 'xcve', 'xcvp', 'xqvc', 'xqvm'):
			# Versal devices
			temp_variant = dev_dict['temp_variants'][0]['grade'].lower()
			part_number = f'{name}-{package}{grade}-{temp_variant}-S'
		elif name == 'xq7z045' and package == 'rfg676':
			# The xq7z045-rfg676 only has the -1QL speed grade
			part_number = f'{name}{package}-1QL'
		elif name[:4] in ('xck26'):
			# Kria-SOMs
			temp_variant = dev_dict['temp_variants'][0]['grade'].lower()
			part_number = f'{name}-{package}{grade}-{temp_variant}'
		else:
			part_number = f'{name}{package}{grade}'

		return part_number

	for (fam, pkgs) in f_map.items():
		log.info(f'Family {fam} has {len(pkgs)} known packages')
		for (pkg, devs) in pkgs.items():
			script = (TILE_SCRIPT_DIR / f'{fam}-{pkg}.tcl')
			with open(script, 'w') as s:
				s.write(render_template(
					'extract_tilemap',
					filters      = {
						'part_number': _gen_partnumber,
					},
					family       = fam,
					device_names = devs,
					devices      = norm['devices'],
					package      = pkg
				))
				s.flush()

			scripts.append(str(script.resolve()))

	with multiprocessing.Pool(args.parallel_jobs) as p:
		p.map(_run_vivado, scripts)


def normalize_tiles(args, progress):
	families = ('spartan', 'artix', 'virtex', 'kintex', 'zynq', 'versal')

	norm = {
		'generated': datetime.now().isoformat(),
		'families' : {}
	}

	files = tuple(TILE_OUTPUT_DIR.glob('*.json'))
	log.info(f'Collected {len(files)} files')
	family_task = progress.add_task('Normalizing Families', total = len(families))

	coord_re = re.compile(r'(?P<x>X\d+)(?P<y>Y\d+)')

	for f in families:
		progress.update(family_task, advance = 1, description = f'Normalizing family \'{f}\'')
		log.info(f'Normalizing family \'{f}\'')
		norm['families'][f] = {
			'tile_maps'    : {},
			'clock_regions': {},
		}
		f_files = tuple(filter(lambda file: file.name[:len(f)] == f, files))
		log.info(f'Family has {len(f_files)} files')
		file_task = progress.add_task('Normalizing files', total = len(f_files))
		for file in f_files:
			progress.update(file_task, advance = 1, description = f'{file.name}')
			with open(file, 'r') as jf:
				data = json.load(jf)
			assert data['family'] == f
			data.pop('family', None)

			tile_map_task = progress.add_task('De-duping tile maps and clock regions', total = 2*len(data))
			for (part, maps) in data.items():

				tile_map  = filter(lambda i: i != '', maps['tiles'])
				clock_map = filter(lambda i: i != '', maps['clock_regions'])

				# Collect and de-dupe tile maps
				t_map = {}
				for (t_key, coord) in map(lambda tile: tile.rsplit('_', 1), tile_map):
					coords = coord_re.match(coord)
					t_map.setdefault(t_key, []).append(
						{'x': int(coords.group('x')[1:]), 'y': int(coords.group('y')[1:])}
					)

				if part not in norm['families'][f]['tile_maps']:
					norm['families'][f]['tile_maps'][part] = [t_map]
				else:
					has_match = False
					for tile_maps in norm['families'][f]['tile_maps'][part]:
						if tile_maps.keys() != t_map.keys():
							continue

						if all(
							len(
								frozenset(map(lambda t: frozenset(t.items()), t_map[key])) ^
								frozenset(map(lambda t: frozenset(t.items()), tile_list))
							) == 0

							for (key, tile_list) in tile_maps.items()
						):
							has_match = True
							break

					if not has_match:
						norm['families'][f]['tile_maps'][part].append(t_map)

				progress.update(tile_map_task, advance = 1)

				# Collect and de-dupe clock regions per-part
				clk_regions = list()
				for r in clock_map:
					m = coord_re.match(r)
					clk_regions.append(
						{'x': int(m.group('x')[1:]), 'y': int(m.group('y')[1:])}
					)

				if part not in norm['families'][f]['clock_regions']:
					norm['families'][f]['clock_regions'][part] = [clk_regions]
				else:
					clk_r = frozenset(map(lambda d: frozenset(d.items()), clk_regions))

					if not any(
						len(clk_r ^ frozenset(map(lambda d: frozenset(d.items()), clk_rs))) == 0
						for clk_rs in norm['families'][f]['clock_regions'][part]
					):
						norm['families'][f]['clock_regions'][part].append(clk_regions)

				progress.update(tile_map_task, advance = 1)
			progress.update(tile_map_task, visible = False)
		progress.update(file_task, visible = False)
	progress.update(family_task, visible = False)

	return norm

def write_tile_maps(args, norm_tiles):
	families = ('spartan', 'artix', 'virtex', 'kintex', 'zynq', 'versal')

	for family in families:
		f_tile_map = {
			'generated': norm_tiles['generated'],
			**norm_tiles['families'][family]
		}

		log.info(f'Writing tile maps for \'{family}\'')
		with open(((XYLENE_DB_DIR / family) / 'tile_maps.json.zst'), 'w') as index:
			json.dump(f_tile_map, index)

def mkpartindex_main(args):

	with Progress(
		SpinnerColumn(),
		TextColumn('[progress.description]{task.description}'),
		BarColumn(bar_width = None),
		transient = True
	) as progress:
		task = progress.add_task('Extracting Parts', start=False)
		if not RAW_PART_INDEX.exists():
			dump_parts = (XYLENE_WORKING_DIR / 'dump_parts.tcl')
			if not dump_parts.exists():
				with dump_parts.open('w') as tcl_script:
					tcl_script.write(render_template('dump_parts'))

			log.info('Generating raw part index, this will take a [yellow][i]while[/][/]', extra = { 'markup': True })
			with Vivado(log = False, journal = False) as vivado:
				success = vivado.run_tcl(
					tcl_file = str(dump_parts.resolve()),
					tcl_args = [str(RAW_PART_INDEX)]
				)

				if not success:
					log.error('Unable to extract parts database')
					log.info('Writing process output logs')

					RAW_PART_INDEX.rename(f'{RAW_PART_INDEX}.bad')

					with open((XYLENE_WORKING_DIR / f'{dump_parts.name}.stdout'), 'w+') as stdout:
						stdout.write(vivado.stdout.decode('utf-8'))

					with open((XYLENE_WORKING_DIR / f'{dump_parts.name}.stderr'), 'w+') as stderr:
						stderr.write(vivado.stderr.decode('utf-8'))
					return 1

		progress.update(task, description = 'Normalizing index')
		norm = normalize(args)

		progress.update(task, description = 'Writing finalized monolithic JSON index')
		with open(NORMALIZED_INDEX, 'w') as index:
			json.dump(norm, index)

		progress.update(task, description = 'Writing independent family JSON indices')
		write_family_json(args, norm)

		if not args.skip_tiles:
			progress.update(task, description = 'Extracting tile map and clock regions')
			extract_tiles(args, norm)

			progress.update(task, description = 'Normalizing tiles and clock regions')
			norm_tiles = normalize_tiles(args, progress)

			progress.update(task, description = 'Writing normalized tile maps and clock regions')
			write_tile_maps(args, norm_tiles)
		else:
			log.warning('Skipping tile map and clock region extraction')


	return 0


def mkpartindex_parser(parser):
	parser.add_argument(
		'--parallel-jobs', '-P',
		type    = int,
		default = (multiprocessing.cpu_count() // 4),
		help    = 'Number of parallel instances of Vivado to run'
	)

	parser.add_argument(
		'--skip-tiles', '-T',
		action = 'store_true',
		help   = 'Skip generating the tile and clock maps'
	)

if __name__ == '__main__':
	sys.exit(main(
		mkpartindex_main,
		'mkpartindex',
		'Generate the Xylene device index',
		mkpartindex_parser
	))
